{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "#%matplotlib widget\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import json\n",
    "\n",
    "from iterativenn.nn_modules.MaskedLinear import MaskedLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intake the mass spec data csv\n",
    "df = pd.read_csv('/home/kjmetzler/iterativenn/notebooks/START_HERE/4-data/mass_spec_data(in).csv',header=0)\n",
    "\n",
    "df = df.drop('chem_name', axis=1)\n",
    "\n",
    "#Split the data frame into two\n",
    "labels = df.iloc[:,-50:]\n",
    "chems = df.iloc[:,0:df.shape[1]-50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create noisy labels\n",
    "def add_noise(dataframe):\n",
    "    lab = dataframe.iloc[:, -50:]  # Last 50 columns\n",
    "    chemicals = dataframe.iloc[:, 0:dataframe.shape[1] - 50]  # Other columns\n",
    "    \n",
    "    # Create noise for labels, making sure they sum to 1 across each row\n",
    "    noise_labels = np.random.rand(lab.shape[0], lab.shape[1])  # Random noise between 0 and 1\n",
    "    noise_labels = noise_labels / noise_labels.sum(axis=1, keepdims=True)  # Normalize to sum to 1 per row\n",
    "    \n",
    "    # Concatenate the chemical data with the normalized noise\n",
    "    new_start = pd.concat([chemicals, pd.DataFrame(noise_labels)], axis=1, ignore_index=True)\n",
    "    \n",
    "    return new_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the size of each input vector based on the second dimension of 'x_start'.\n",
    "x_size = df.shape[1]\n",
    "y_size = df.shape[1]\n",
    "# Determine the number of data points (rows in 'x_start').\n",
    "data_size = df.shape[0]\n",
    "\n",
    "# Define the size of blocks within the mask for each row group. \n",
    "# This is related to the 'Experimental Models' sheet in Kevin's Drive.\n",
    "row_sizes = [chems.shape[1],x_size-chems.shape[1]] \n",
    "\n",
    "# Define the size of blocks within the mask for each column group.\n",
    "col_sizes = [x_size]\n",
    "\n",
    "# Specify the types of blocks to be used in the masked linear layer. This configuration controls the structure of the weight matrix.\n",
    "# 'D' indicates a diagonal block, while \n",
    "# 'W' indicates a dense block.\n",
    "# 'S' indicates a sparse block, with n trainable entries distributed randomly, 'S=...'.\n",
    "#  0  indicates a zero block \n",
    "block_types = [['S=800'],['S=400']]\n",
    "\n",
    "# Define initialization types for the blocks.\n",
    "# This dictates how the weight matrices within each block are initially set up.\n",
    "# 'G' indicates a Gaussian Distribution, 1 indicates the identity, 0 indicates a zero block.\n",
    "initialization_types = [[1],['G']]\n",
    "\n",
    "# Specify which blocks are trainable.\n",
    "# A value of 0 indicates the block is not trainable, while a value of 1 indicates it is trainable.\n",
    "trainable = [[1],[1]]\n",
    "\n",
    "# Initialize a MaskedLinear layer with specified configurations.\n",
    "chem_ml = MaskedLinear(x_size, x_size, bias=True)\n",
    "chem_MaskLin = chem_ml.from_description(row_sizes=row_sizes,\n",
    "                                          col_sizes=col_sizes,\n",
    "                                          block_types=block_types,\n",
    "                                          initialization_types=initialization_types,\n",
    "                                          trainable=trainable)\n",
    "\n",
    "# Construct the neural network using sequential layers.\n",
    "# Here, 'rand_MaskLin' specifies the custom configured MaskedLinear layer, interleaved with LeakyReLU activation functions.\n",
    "chem_INN = torch.nn.Sequential(chem_MaskLin, \n",
    "                               nn.LeakyReLU(),\n",
    "                               chem_MaskLin,\n",
    "                               nn.LeakyReLU(),\n",
    "                               chem_MaskLin)\n",
    "\n",
    "# Number of LeakyReLU activation functions used in the network.\n",
    "# This is used for determining the inverse function.\n",
    "num_relus = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a pandas dataframe into a pytorch tensor\n",
    "def df_to_tensor(df):\n",
    "    return torch.tensor(df.values, dtype=torch.float32)\n",
    "\n",
    "# Make two pytorch tensor datasets from the start and target data\n",
    "x_start_tensor = df_to_tensor(add_noise(df))\n",
    "x_target_tensor = df_to_tensor(df)\n",
    "\n",
    "# a dataloader which returns a batch of start and target data\n",
    "class Data(torch.utils.data.Dataset):\n",
    "    def __init__(self, x_start, x_target):\n",
    "        self.x_start = x_start\n",
    "        self.x_target = x_target\n",
    "    def __len__(self):\n",
    "        return len(self.x_start)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_start[idx], self.x_target[idx]\n",
    "    \n",
    "train_data = Data(x_start_tensor, x_target_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "criterion2= torch.nn.MSELoss() \n",
    "optimizer = torch.optim.Adam(chem_INN.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum number of epochs for training.\n",
    "max_epochs = 3000\n",
    "# Initialize an empty list to keep track of the loss values after each epoch.\n",
    "loss_graph = []\n",
    "\n",
    "# Begin the training loop.\n",
    "for epoch in range(max_epochs):\n",
    "    # Iterate over batches of data in the training loader.\n",
    "    for batch_idx, (start, target) in enumerate(train_loader):\n",
    "        # Reset the gradients of all model parameters to zero.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Pass the batch through the model to get predictions.\n",
    "        set = chem_INN(start)\n",
    "        soft_guess = torch.softmax(set[:,-50:],dim=1)\n",
    "        mass_spec = set[:,:set.shape[1]-50]\n",
    "        \n",
    "        # Initialize the loss for the current batch.\n",
    "        loss = 0\n",
    "        # Calculate the loss\n",
    "        loss += 10*criterion(soft_guess, target[:,-50:])\n",
    "        loss += criterion2(mass_spec, target[:,:target.shape[1]-50]) #need to change the target to include the mass spec\n",
    "\n",
    "        # Compute the gradients based on this loss.\n",
    "        loss.backward()\n",
    "        # Update the parameters of the model according to the optimization strategy.\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Optionally, save the current state of the model.\n",
    "        state = chem_INN.state_dict()\n",
    "        \n",
    "    # After each epoch, record the loss for plotting.\n",
    "    loss_graph.append(loss.item())\n",
    "    # Optionally, print the training progress every 10 epochs.\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Batch {batch_idx}, Loss {loss.item()}')\n",
    "\n",
    "# After training, plot the recorded loss values.\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, len(loss_graph)), loss_graph)\n",
    "plt.semilogy()  # Use a logarithmic scale for the y-axis to better visualize changes.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "pred_output = chem_INN(x_start_tensor)\n",
    "pred_last_50 = pred_output[:, -50:]  # Last 50 elements\n",
    "\n",
    "target_last_50 = x_target_tensor[:,-50:]\n",
    "\n",
    "pred_classes = torch.argmax(pred_last_50, dim=1).cpu().numpy()\n",
    "target_classes = torch.argmax(target_last_50, dim=1).cpu().numpy()\n",
    "\n",
    "conf_matrix = confusion_matrix(target_classes, pred_classes)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GLOBAL SLURM 120 Mins NOGPU 4 CPUs (via turing.wpi.edu)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
