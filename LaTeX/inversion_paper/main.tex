%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The source files were:
%%
%% samples.dtx  (with options: `all, proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright, see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source, see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript, please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera-ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\usepackage{xcolor}
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[KDD 2025]{Make sure to enter the correct
  conference title from your rights confirmation email}{August 03--07,
  2025}{Toronto, ON, Canada}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear styles, that include
%% support for advanced citation of software artifact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Iterative Neural Networks for Data Generation}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Kevin James Metzler}
%% \authornote{Both authors contributed equally to this research.}
\email{kjmetzler@wpi.edu}
%%\orcid{1234-5678-9012}
%%\author{G.K.M. Tobin}
%%\authornotemark[1]
%%\email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Worcester Polytechnic Institute}
  \city{Worcester}
  \state{Massachusetts}
  \country{USA}
}

\author{Zahra Bolkheiri}
\email{zahrabolkhairi@gmail.com}
\affiliation{%
  \institution{Persian Gulf University}
  \city{Worcester}
  \state{Massachusetts}
  \country{USA}
}

\author{Cate Dunham}
\email{cmdunham@wpi.edu}
\affiliation{%
  \institution{Worcester Polytechnic Institute}
  \city{Worcester}
  \state{Massachusetts}
  \country{USA}
}

\author{Randy Paffenroth}
\email{rcpaffenroth@wpi.edu}
\affiliation{%
  \institution{Worcester Polytechnic Institute}
  \city{Worcester}
  \state{Massachusetts}
  \country{USA}
}

%\author{Aparna Patel}
%\affiliation{%
% \institution{Rajiv Gandhi University}
% \city{Doimukh}
% \state{Arunachal Pradesh}
% \country{India}}

%\author{Huifen Chan}
%\affiliation{%
%  \institution{Tsinghua University}
%  \city{Haidian Qu}
%  \state{Beijing Shi}
%  \country{China}}

%\author{Charles Palmer}
%\affiliation{%
%  \institution{Palmer Research %Laboratories}
%  \city{San Antonio}
%  \state{Texas}
%  \country{USA}}
%\email{cpalmer@prl.com}

%\author{John Smith}
%\affiliation{%
%  \institution{The Th{\o}rv{\"a}ld Group}
%  \city{Hekla}
%  \country{Iceland}}
%\email{jsmith@affiliation.org}

%\author{Julius P. Kumquat}
%\affiliation{%
%  \institution{The Kumquat Consortium}
%  \city{New York}
%  \country{USA}}
%\email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used on the page
%% headers. Often, this list is too long and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{KJ. Metzler et al.}

%%
%% The abstract is a summary of the work to be presented in the
%% article.
\begin{abstract}
The emergence of DALL-E in 2021 revolutionized generative modeling by employing large-scale transformers to excel in text-to-image synthesis, outperforming GANs in detail and versatility. Iterative Neural Networks (INNs) represent a novel approach to generative tasks, treating neural networks as dynamical systems by enforcing architectural constraints, such as maintaining the same input and output spaces. Utilizing masked linear layers, INNs introduce sparsity and interpretability, iteratively refining outputs toward target manifolds with enhanced stability and reduced overfitting.

This study explores INNs' capacity to address limitations in existing diffusion methods like DALL-E by enabling visible intermediate states and leveraging custom loss functions to guide inputs towards the "attracting manifold." We present a comprehensive comparison of four distinct approaches to achieving bidirectional invertibility in diffusion models: implicit (dual-model), semi-implicit (z-switch with roundtrip consistency), semi-explicit (SVD-based pseudo-inverse), and explicit (mathematical network inversion). Additionally, we introduce same-class convex combination sampling, which preserves semantic structure during diffusion by interpolating only between samples of the same class.

A critical contribution addresses the structural collapse problem in traditional noise-prediction diffusion models. We demonstrate that parameterizing diffusion models to directly predict clean data ($\mathbf{x}_0$ prediction) rather than noise preserves complex non-Gaussian manifold geometry that noise-based approaches collapse to Gaussian-like distributions. Applied to ion mobility spectrometry (IMS) latent representations across 8 chemicals, this approach achieves near-perfect statistical matching (means within 0.08, standard deviations within 7.4\%) while maintaining tendril-like geometric structures validated through blind eye tests with domain experts.

To validate our approaches, mass spectrometry data from 50 chemicals, IMS data from 8 chemicals, and MNIST1D data were used to synthesize spectra, assessing their utility through classification tasks and geometric analysis. Our experimental framework comprises systematic experiments comparing loss functions (FID vs. KL divergence), sampling methods (Gaussian vs. same-class convex), training methods, and diffusion parameterizations. The findings underscore INNs' adaptability, efficiency, and potential for advancing synthetic data generation and interpretability in complex low-data domains, with convex combinations achieving F1-scores of 0.76 compared to 0.65 with real data alone for mass spectrometry classification, and x$_0$ prediction generating photorealistic IMS spectra indistinguishable from real measurements.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}

\end{CCSXML}

%\ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
%\ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
%\ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
%\ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
% \keywords{Do, Not, Us, This, Code, Put, the, Correct, Terms, for,
%  Your, Paper}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
%\begin{teaserfigure}
%  \includegraphics[width=\textwidth]{sampleteaser}
%  \caption{Seattle Mariners at Spring Training, 2010.}
%  \Description{Enjoying the baseball game from the third-base
%  seats. Ichiro Suzuki is preparing to bat.}
%  \label{fig:teaser}
%\end{teaserfigure}

\received{1 February 2025}
\received[revised]{12 March 20XX}
\received[accepted]{5 June 20XX}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

In data-driven scientific research, the availability of high-quality data often determines the success of analytical and predictive models. However, many domains, such as ion mobility spectrometry (IMS) and mass spectrometry (MS), operate in low-data environments where acquiring sufficient data is challenging due to the cost, complexity, or time required for experiments. This limitation hinders the development of robust machine learning models, particularly for tasks such as chemical identification, anomaly detection, and classification. Generative models provide a promising avenue for addressing these challenges by synthesizing high-quality domain-relevant data that augment existing datasets.\cite{Vadakedath2022MS}

Traditional generative architectures, such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), have shown promise in data augmentation tasks but often struggle in low-data environments. These models require substantial data to avoid overfitting and generate realistic, diverse outputs. Recent advances in iterative neural networks (INNs) offer a powerful alternative. Using iterative refinement processes, INNs learn representations that are flexible and capable of capturing fine-grained details in complex datasets. This makes them well-suited for data generation tasks in low-data settings, where the iterative approach enables the network to generalize effectively from limited examples.\cite{hershey2024rethinking}

To evaluate the utility of INNs for data generation, we focused on three datasets: ion mobility spectra, mass spectra, and the MNIST dataset. IMS and MS datasets are particularly well-suited to highlight the capabilities of INNs due to their inherent complexity and the scarcity of labeled data. MNIST, a widely used benchmark data set in generative modeling, serves as a baseline to validate our approach against existing methods. The central goal of this work is to demonstrate how INNs can generate synthetic data that are realistic and utility-driven, that is, data that meaningfully improve downstream tasks such as classification or clustering when combined with real-world data.\cite{Vadakedath2022MS}

Unlike traditional neural networks, iterative neural networks refine their outputs through multiple iterations, effectively incorporating feedback from intermediate steps. This iterative refinement allows for better handling of noisy or incomplete data and facilitates domain-specific constraints, such as the non-negativity and bounded nature of spectra. In this paper, we explore the advantages of INNs over traditional architectures, highlighting their versatility and adaptability to generate synthetic data tailored to the unique characteristics of each data set.\cite{hershey2024rethinking}

\subsection{Contributions}

This work makes several key contributions to the field of generative modeling with INNs:

\begin{enumerate}
    \item \textbf{Comprehensive Inversion Comparison}: We systematically evaluate four distinct approaches to achieving bidirectional invertibility in diffusion models—implicit, semi-implicit, semi-explicit (SVD), and explicit (mathematical)—providing the first rigorous comparison of these methods in the context of INNs.
    
    \item \textbf{Same-Class Convex Combination Sampling}: We introduce a novel sampling strategy that preserves semantic class structure during diffusion by constraining convex combinations to same-class pairs, improving the quality of generated synthetic data.
    
    \item \textbf{X$_0$ Prediction for Manifold Preservation}: We demonstrate that parameterizing diffusion models to directly predict clean data rather than noise enables preservation of complex non-Gaussian manifold geometry. This addresses the structural collapse problem where noise-prediction models reduce tendril-like structures to Gaussian blobs despite matching statistical moments.
    
    \item \textbf{Class-Conditioned Multi-Chemical Generation}: We develop a single diffusion model conditioned on molecular SMILE embeddings and class labels that successfully generates synthetic data for 8 different chemicals, achieving parameter efficiency and transfer learning benefits without requiring per-chemical models.
    
    \item \textbf{Comprehensive Experimental Framework}: We establish systematic evaluation protocols comparing loss functions, sampling methods, and training approaches on specialized scientific data (IMS, mass spectrometry) and benchmark data (MNIST1D).
    
    \item \textbf{Practical Validation}: We demonstrate significant improvements in downstream classification tasks (17\% higher F1-scores for mass spectrometry) and validate photorealistic generation quality through blind eye tests with domain experts for IMS data.
\end{enumerate}

%\section{Synthetic Data Generation}

%\section{DALL-E}

%The introduction of DALL-E in 2021 by Ramesh et al. marked a transformative moment in the field of generative modeling, pushing the boundaries of synthetic data generation beyond what traditional GANs had achieved. DALL-E, a variant of the GPT architecture, demonstrated how large-scale transformer models could outperform GANs in generating highly detailed synthetic data, particularly in text-to-image tasks.\cite{ramesh2021zero}


\section{Iterative Neural Networks}

Iterative Neural Networks, as the name implies, are neural networks that we treat as dynamical systems. To do this, we enforce specific architectural requirements such as the input must be in the same space as the output so that it can be fed back into the network for a second iteration.

\begin{align*}
    \sigma (Wx_0 + b) &= x_1 \\
    \sigma (Wx_1 + b) &= x_2
\end{align*}

This is not to be confused with Recurrent Neural Networks (RNNs), as we are not passing a hidden state to the next recurrent unit along with a new input, but the whole output tensor.

To do this, we use modified layers of the network such that we control which segments of the matrix layer are trainable and which are not.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{mlp-to-INN.png}
  \caption{An example of an MLP represented as an INN.}
  \Description{A woman and a girl in white dresses sit in an open car.}
\end{figure}

Hershey et al. (2024) demonstrated that we can construct INNs using masked linear layers and that they can represent any neural network, provided the correct mask and sufficient iterations of the network. This finding underscores the flexibility of INNs, which can adapt to diverse tasks and constraints by tailoring the mask structure. Their versatility makes them a valuable tool for constructing efficient and robust models. \cite{hershey2024rethinking}

\subsection{Masked Linear}

Masked linear layers are essential for introducing sparsity and interpretability in (INNs). These layers restrict connectivity by zeroing out specific weights based on a predefined mask, reducing computational complexity while aligning the network's structure with desired dynamical properties.

In INNs, masked layers introduce incremental updates to the data manifold during each iteration. Rather than immediately mapping inputs to the target manifold, the network evolves gradually, enhancing stability during training and reducing overfitting.

From a dynamical systems perspective, masked layers emulate discretized flows, where each iteration represents a step along a trajectory. Adjusting the sparsity pattern allows for the regulation of the system’s behavior by emphasizing or suppressing data dimensions. This is advantageous for tasks like denoising and inverse problems, where it is necessary to retain critical information.\cite{hershey2024rethinking} 

%\subsection{Comparison with DALL-E}

%One of the main issues with the DALL-E approach is a lack of insight into the hidden layers of the model. Because of the typical neural network setup, each hidden layer can exist in any space so long as it is useful for the model to get to its final prediction of what the generated data should look like. This black-box structure prevents us from visualizing the convergence of the input towards the output.

%Our proposed solution to this is to use INNs to force the input, each intermediate state, and the output to all be within the same space. The input and output spaces are required to be the same so that we can feed the output back into the network as the new input. Meanwhile, the intermediate states are no longer hidden. They are the intermediate outputs from our Dynamical System such that the distance, however we choose to define it, is reduced with each step of the network. \cite{hershey2024rethinking} 

%This approach aims to force the network to bring input that is far from the “attracting manifold” close to it, and inputs that are nearby to stay nearby.

\subsection{INN Loss Functions}

Ho et al., in their 2020 paper on diffusion models, found that they generally struggle to diffuse data that doesn’t have enough noise because it is already too close to the idealistic “attracting manifold.” Thus, our approach additionally aims to solve this with iterative methods. Since INNs aim for convergence towards this manifold over several iterations rather than trying to find a universal function from the input space to the manifold space, we can use loss functions that incentivize input that is already close to the manifold to stay near the manifold and input that is far from this manifold to take larger steps towards it. This is a major problem for larger diffusion methods like DALL-E, as they require that their input is almost pure noise along with the text to be able to generate a proper image.\cite{ho2020denoising}\cite{ramesh2021zero}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{ho-diffusion.png}
    \caption{Diagram of denoised interpolation from a diffused source. Picture from Ho et al.\cite{ho2020denoising}}
    \label{fig:enter-label}
\end{figure}

For our INNs, we will use the following schema to train our model:

\begin{equation}
    L = \sum_{i=0}^n l(x_i, x_t)
\end{equation}

Where $l(x_i, x_t)$ is the loss from iteration $i$ compared to the target data point, $x_t$, that we diffused to create the initial input.

\section{Decoupled Autoencoders}
% I wasn't sure how to reference my ICMLA paper since you aren't on it? I figured, "our previous work" could still be appropriate since both Randy and I are on that paper? Feel free to change as you see fit, though. 
As an additional point of comparison, we utilize the decoupled autoencoder structure introduced in \textcolor{blue}{our previous work} \cite{dunham2024oracle}. 
In a standard autoencoder, encoder $E$ learns a compressed representation $\mathbf{h}$ of the data $\mathbf{x}$ that is passed to the decoder $D$. The model aims to minimize
$$\| \mathbf{x} - D(E( \mathbf{x} ))\| \; \text{or} \; \| \mathbf{x} - D(\mathbf{h})\|,$$ 
the difference between the input data and the model's output.

The central difference between our decoupled autoencoder and a standard model is that, rather than being learned by the encoder from the data, $\mathbf{h}$ is provided to our model by an external chemistry model. The external model, ChemNet, was trained to predict 6,000 properties for each of the 3.6 million chemical structures in its dataset \cite{preuer2019frechet}. Due to the size of its training data and the complexity of its training task, the chemical embeddings ChemNet creates contain a wealth of chemistry information that would not be attainable from the small datasets we work with. 

For a chemical $c$, we determine $c$'s ChemNet embedding, a $512$-dimensional vector $\mathbf{h}_c \in \mathbb{R}^{512}$. We train an encoder to map sensor data for chemical $c$, $\mathbf{x}_c$, to $\mathbf{h}_c$, minimizing
\begin{equation}
\|\mathbf{h}_c - E(\mathbf{x}_c)\|. 
\label{enc loss function}
\end{equation}

We then train a decoder to map the embeddings our encoder created for chemical $c$, $\hat{\mathbf{h}}_c$, back to $\mathbf{x}_c$. The decoder aims to minimize 
\begin{equation}
\|\mathbf{x}_c - D(\hat{\mathbf{h}}_c)\|.
\label{dec loss function}
\end{equation}

Once both pieces of the model are trained, synthetic data can be generated by selecting new $\hat{\mathbf{h}}_c$ to hand to the decoder. A more detailed description of the synthetic data generation process is provided in \cite{dunham2024oracle}.

\section{Datasets}

\subsection{Mass Spectrometry}

Mass spectrometry (MS) is an analytical technique used to identify and quantify molecules based on their mass-to-charge ratio. By ionizing chemical compounds and separating the resulting ions in a mass analyzer, MS provides a unique "mass spectrum" for each molecule, serving as its chemical fingerprint. This spectrum is a graphical representation of ion intensity as a function of the mass-to-charge ratio, offering insights into the molecular structure and composition.\cite{Vadakedath2022MS}

For this study, mass spectra data were collected for 50 different chemicals. The dataset comprises 572 samples, each represented by 915 features (mass-to-charge ratios) derived from the mass spectrum. The dataset includes a one-hot encoded classification space with 50 classes, corresponding to the distinct chemicals under study.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{Dimethylpentacosane.png}
    \caption{A sample mass spectrum for Dimethylpentacosane.}
    \label{fig:enter-label}
\end{figure}

Mass spectrometry has diverse applications across scientific disciplines, including:

\begin{itemize}
    \item \textbf{Chemical Identification}: MS enables precise identification of unknown substances by matching a molecule's mass spectrum against known spectra in databases. This capability is essential for fields such as environmental science, pharmaceuticals, and chemical engineering.\\
    \item \textbf{Defense Threat Reduction Agency (DTRA) Applications}: MS is pivotal in security-related applications, including the detection of hazardous chemicals, explosives, and biological agents. Its sensitivity and specificity make it a valuable tool for rapid threat assessment and mitigation.
\end{itemize}

According to Vadakedath et al. (2022), MS’s versatility lies in its ability to analyze complex mixtures, characterize biomolecules, and support a wide range of research and industrial applications. \cite{Vadakedath2022MS}

\subsection{Ion Mobility Spectrometry}
Another common analytical technique in chemistry is ion mobility spectrometry (IMS), which examines how ions separate based on their velocity in an electric field \cite{hill1990ion}. Graphical representations of IMS spectra, as shown in FIGURE SOMETHING, show ion intensity, the abundance of ions detected, plotted against drift time, the time taken for ions to travel a specific distance. IMS data can be used alone or in conjunction with mass spectrometry spectra to study and identify analytes \cite{dodds2019ion}. 

The ion mobility dataset used in this study contains over 350,000 samples divided across 8 chemicals/classes. An imbalance between the classes was observed, with samples from the majority class being 13 times more abundant than those from the minority class, but this was not adjusted as part of this study. 

Samples in the dataset are comprised of two parts: a positive spectrum measuring ion intensity and drift time for positively charged ions; and a negative spectrum that represents the corresponding metrics for negatively charged ions. Each sample consists of 837 drift times for each the positive and the negative spectrum, resulting in a total of 1,674 features ($837$ drift times $\times 2$ spectra).


\section{Methodology}

\subsection{Evaluation Framework for Synthetic Spectra}

To evaluate whether our INN can generate useful synthetic spectra for the 50 chemicals, we need a method to assess the "utility" of these synthetic spectra. Specifically, we test whether training a generic classifier on the synthetic spectra improves its ability to identify real spectra compared to training solely on real spectra.

The first step involves diffusing the real spectra to train the network to reconstruct the original data. The original dataset is split to ensure that no spectra used for network training are later used for training or testing the classifier. To generate diffused spectra, we explore two approaches:

\textbf{Gaussian Diffusion:} Gaussian noise is applied to modify the peaks in the spectra. Care is taken to ensure that the diffused spectra remain physically valid. Negative values are avoided, and any values above the total mass-to-charge ratio are left as zero since they do not contain meaningful information.

\textbf{Same-Class Convex Combinations:} For each spectrum of chemical class $c$, we randomly select another spectrum from the same class and compute:
\begin{equation}
    x_{\text{noisy}} = \alpha x_i + (1-\alpha) x_j, \quad \alpha \sim \text{Uniform}(0,1)
\end{equation}
where both $x_i$ and $x_j$ belong to class $c$. This preserves class-specific features while introducing controlled variation.

Multiple diffusions are performed on the same spectra using increasing noise levels. This ensures that the network learns the general structure of the data rather than overfitting to specific perturbations. The diffusion process is described by:
\begin{equation}
    x_\alpha = \begin{bmatrix}
        \alpha_1\\
        \vdots\\
        \alpha_m
    \end{bmatrix} \circ 
    \begin{bmatrix}
        \varepsilon_{1,i}\\
        \vdots\\
        \varepsilon_{m,i}
    \end{bmatrix} = 
    \begin{bmatrix}
        \tilde \alpha_1\\
        \vdots\\
        \tilde \alpha_m
    \end{bmatrix} = \tilde x_{\alpha, i}
\end{equation}
where $x_\alpha$ represents a real spectrum with mass-to-charge ratios $\alpha_{k,i}$ for $k = 1, 2, \dots, m$. For Gaussian diffusion, noise terms $\varepsilon_{k,i} \sim N(1, \frac{i+1}{2n})$ are sampled from a Gaussian distribution, with $n$ being the total number of diffusion steps and $i = 0, \dots, n$. For convex combinations, $\varepsilon$ represents the convex interpolation with another sample of the same class.

\subsection{Bidirectional Training}

During training, we simultaneously learn both forward (denoising) and inverse (mapping to prime space) transformations. All information about the chemical labels is intentionally removed during the forward pass to ensure that the network learns both the structure of real spectra and how to classify them without relying on predefined labels. This prevents self-validation bias, ensuring the generalizability of the learned representations.

For the inverse direction, we define deterministic "prime inverse" patterns for each chemical class—structured noise distributions that the model learns to generate from clean spectra. This bidirectional training enables the network to understand both data generation and recognition.

\subsection{Comparative Evaluation}

Once the network is trained using one of the four inversion methods (Implicit, Semi-Implicit, Semi-Explicit, or Explicit), we construct a test dataset of synthetic spectra. This dataset is then used to train a random forest classifier to evaluate the effectiveness of the synthetic spectra in improving classification accuracy for real spectra.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Step-by-step.png}
    \caption{Caption}
    \label{fig:enter-label}
\end{figure}

\section{Results}

\begin{table*}
  \caption{Classification Results from Real and Synthetic Training Sets of Mass Spectrometry Data}
  \label{tab:commands}
  \begin{tabular}{l|l|l|l|l}
    \toprule
    Macro Averages & Precision & Recall & F1-Score & Accuracy\\
    \midrule
    Real Spectra & 0.71 & 0.66 & 0.65 & 0.76 \\
    Diffused then Iterated Spectra & 0.77 & 0.72 & 0.72 & 0.78 \\
    Convex Combs. of Real Spectra & 0.82 & 0.76 & 0.76 & 0.80 \\
    Both Treatments & 0.81 & 0.75 & 0.74 & 0.77 \\
    \midrule
    Weighted Averages & Precision & Recall & F1-Score & Accuracy\\
    \midrule
    Real Spectra & 0.84 & 0.76 & 0.75 & 0.76 \\
    Diffused then Iterated Spectra & 0.85 & 0.78 & 0.78 & 0.78 \\
    Convex Combs. of Real Spectra & 0.87 & 0.80 & 0.80 & 0.80 \\
    Both Treatments & 0.87 & 0.77 & 0.78 & 0.77 \\
    \bottomrule
  \end{tabular}
\end{table*}

\begin{table*}
  \caption{Classification Results from Real and Synthetic Training Sets of MNIST Data}
  \label{tab:commands}
  \begin{tabular}{l|l|l|l|l}
    \toprule
    Macro Averages & Precision & Recall & F1-Score & Accuracy\\
    \midrule
    Real Images & 0.84 & 0.85 & 0.84 & 0.84 \\
    Diffused then Iterated Images & 0.84 & 0.85 & 0.84 & 0.84 \\
    Convex Combs. of Real Images & 0.83 & 0.83 & 0.82 & 0.82 \\
    Both Treatments & 0.83 & 0.83 & 0.82 & 0.82 \\
    \midrule
    Weighted Averages & Precision & Recall & F1-Score & Accuracy\\
    \midrule
    Real Images & 0.85 & 0.84 & 0.84 & 0.84 \\
    Diffused then Iterated Images & 0.85 & 0.84 & 0.84 & 0.84 \\
    Convex Combs. of Real Images & 0.83 & 0.82 & 0.82 & 0.82 \\
    Both Treatments & 0.83 & 0.82 & 0.82 & 0.82 \\
    \bottomrule
  \end{tabular}
\end{table*}


\begin{table*}
  \caption{Classification Results from Real and Synthetic Training Sets of 10,000 IMS Data}
  \label{tab:commands}
  \begin{tabular}{l|l|l|l|l}
    \toprule
    Macro Averages & Precision & Recall & F1-Score & Accuracy\\
    \midrule
    Real Spectra & 1.00 & 0.99 & 0.99 & 1.00 \\
    Synthetic Spectra & 0.76 & 0.63 & 0.64 & 0.76 \\
    Both & 1.00 & 0.99 & 1.00 & 1.00 \\
    \midrule
    Weighted Averages & Precision & Recall & F1-Score & Accuracy\\
    \midrule
    Real Spectra & 1.00 & 1.00 & 1.00 & 1.00 \\
    Synthetic Spectra & 0.80 & 0.76 & 0.73 & 0.76 \\
    Both & 1.00 & 1.00 & 1.00 & 1.00 \\
    \bottomrule
  \end{tabular}
\end{table*}

\begin{table*}
  \caption{Classification Results from Real and Synthetic Training Sets of 400,000 IMS Data}
  \label{tab:commands}
  \begin{tabular}{l|l|l|l|l}
    \toprule
    Macro Averages & Precision & Recall & F1-Score & Accuracy\\
    \midrule
    Real Spectra & 1.00 & 0.99 & 0.99 & 1.00 \\
    Synthetic Spectra & 0.73 & 0.65 & 0.64 & 0.76 \\
    Both & 1.00 & 0.99 & 0.99 & 0.99 \\
    \midrule
    Weighted Averages & Precision & Recall & F1-Score & Accuracy\\
    \midrule
    Real Spectra & 1.00 & 1.00 & 1.00 & 1.00 \\
    Synthetic Spectra & 0.80 & 0.76 & 0.73 & 0.76 \\
    Both & 1.00 & 0.99 & 0.99 & 0.99 \\
    \bottomrule
  \end{tabular}
\end{table*}


\begin{table*}
  \caption{Classification Results from Real and Synthetic Training Sets of 1,000 IMS Data}
  \label{tab:commands}
  \begin{tabular}{l|l|l|l|l}
    \toprule
    Macro Averages & Precision & Recall & F1-Score & Accuracy\\
    \midrule
    Real Spectra & 0.74 & 0.66 & 0.66 & 0.81 \\
    Diffused then Iterated Spectra & 0.73 & 0.65 & 0.66 & 0.79 \\
    Convex Combs. of Real Spectra & 0.78 & 0.78 & 0.77 & 0.90 \\
    Both Treatments & 0.76 & 0.73 & 0.73 & 0.85 \\
    \midrule
    Weighted Averages & Precision & Recall & F1-Score & Accuracy\\
    \midrule
    Real Spectra & 0.81 & 0.81 & 0.79 & 0.81 \\
    Diffused then Iterated Spectra & 0.77 & 0.79 & 0.76 & 0.79 \\
    Convex Combs. of Real Spectra & 0.87 & 0.90 & 0.88 & 0.90 \\
    Both Treatments & 0.84 & 0.85 & 0.84 & 0.85 \\
    \bottomrule
  \end{tabular}
\end{table*}



This section analyzes the classification results obtained from the synthetic data generated using diffusion, convex combinations, and iterative neural networks. The experiments were conducted on two datasets: mass spectrometry data, characterized by 50 classes and very limited samples (10–20 per class), and MNIST data, with 10 classes and 1000 samples in total (100 per class). The classifiers were trained on various combinations of real and synthetic data, and their performance was evaluated on real test data. Tables~1 and~2 summarize the results for mass spectrometry and MNIST data, respectively.

\subsection{Mass Spectrometry Data}

The mass spectrometry dataset presented challenges due to its high complexity and limited sample size. Classification results, shown in Table~1, demonstrate the impact of the different synthetic data generation techniques.

%\subsubsection{Macro Averages.}
The baseline performance, obtained using only real spectra, achieves moderate scores, with an F1-score of 0.65 and an accuracy of 0.76. These results reflect the difficulty in achieving robust classification with such a small dataset. Training on synthetic data generated via diffusion and iterative methods improves the F1-score to 0.72 and accuracy to 0.78. Convex combinations of real spectra further enhance performance, with an F1-score of 0.76 and accuracy of 0.80. Interestingly, combining both treatments yields results (F1-Score: 0.74, accuracy: 0.77) that are comparable to convex combinations alone, suggesting limited complementarity between the two approaches.

%\subsubsection{Weighted Averages.}
The weighted averages outperform macro averages across all metrics due to the emphasis on the most populated classes. Among the methods, convex combinations yield the best results, with an F1-score and accuracy of 0.80. The diffusion approach follows closely, with an F1-score and accuracy of 0.78, while real spectra lag, emphasizing the importance of synthetic data augmentation.

%\subsubsection{Insights.}
Convex combinations consistently provide the best results, likely due to their ability to expand the data distribution while preserving class-specific features. Diffusion methods moderately improve classification performance by enriching the training data with learned patterns. However, the limited complementarity between diffusion and convex combinations suggests that further optimization of hybrid approaches may be needed.

\subsection{MNIST Data}

The MNIST dataset, with its larger size and simpler structure, provides a contrasting case. Classification results, presented in Table~2, indicate that synthetic data generation methods offer limited improvements compared to the baseline.

%\subsubsection{Macro Averages.}
The baseline performance, achieved using real MNIST images, reaches an F1-score and accuracy of 0.84. Synthetic data generated using diffusion and iterative methods matches the baseline performance, indicating that the synthetic samples align closely with the real data distribution. Convex combinations, however, result in a slight performance drop, with an F1-score of 0.82 and an accuracy of 0.82. Combining both treatments does not provide any additional benefits, yielding identical results to convex combinations alone.

%\subsubsection{Weighted Averages.}
The weighted averages mirror the macro averages closely, with no significant differences across metrics. This stability highlights the balance and homogeneity of the MNIST dataset, which reduces the impact of class-specific variations.

%\subsubsection{Insights.}
For MNIST, real data alone is sufficient to achieve high classification performance. Diffusion-based methods produce synthetic data that is indistinguishable in effectiveness from real data, while convex combinations introduce artifacts that slightly degrade performance. These findings suggest that synthetic data generation is less impactful for datasets with sufficient diversity and simpler structures.

\subsection{Comparative Observations and Recommendations}

%\subsubsection{Dataset-Specific Trends.}
Synthetic data generation is crucial for datasets with limited samples and high complexity, as demonstrated by the mass spectrometry results. In contrast, for larger and simpler datasets like MNIST, synthetic data offers diminishing returns.

%\subsubsection{Effectiveness of Methods.}
For mass spectrometry, convex combinations outperform other methods, highlighting their strength in expanding the data distribution while maintaining class fidelity. Diffusion and iterative methods provide moderate improvements but fall short of the simpler convex combinations approach. For MNIST, diffusion methods succeed in producing synthetic data that matches real data in performance, whereas convex combinations are less effective.

%\subsubsection{Future Directions.}
Future research should explore hybrid methods that selectively apply synthetic data generation techniques based on class-specific characteristics. Furthermore, optimizing the balance between diffusion and convex combinations could further enhance the classification performance across data sets.

\section{Inversion Methods for Bidirectional Diffusion}

A critical challenge in generative modeling is enabling bidirectional transformations: not only generating clean data from noise (forward diffusion) but also mapping clean data to structured noise patterns (inverse diffusion). We investigated four distinct approaches to achieving this invertibility, each offering unique advantages and trade-offs.

\subsection{Implicit Inversion}

The implicit approach trains two separate neural networks: a forward model that denoises inputs and an inverse model that maps clean data to a target noise distribution. Each model specializes in its respective direction, learning independent parameter sets.

\textbf{Architecture:} Two independent PointUNet models with separate parameters.

\textbf{Training:} The forward model minimizes $\mathcal{L}_{\text{forward}} = \mathbb{E}[\|\epsilon - \epsilon_\theta(x_t, t)\|^2]$ where $\epsilon_\theta$ predicts the noise added at timestep $t$. The inverse model minimizes an analogous loss for the reverse transformation.

\textbf{Advantages:} Complete separation of forward and inverse transformations allows each model to specialize without interference. No architectural constraints limit expressiveness.

\textbf{Disadvantages:} Requires double the parameters and training time. No guarantee of consistency between forward and inverse operations.

\subsection{Semi-Implicit Inversion}

The semi-implicit method uses a single model with a directional indicator ($z$-coordinate) to handle both transformations. When $z=0$, the model performs forward denoising; when $z=1$, it performs inverse mapping to the prime noise space.

\textbf{Architecture:} Single PointUNet with conditional input based on $z \in \{0, 1\}$.

\textbf{Training:} Combines four loss terms to enforce bidirectional consistency:
\begin{align}
\mathcal{L} = &\mathcal{L}_{\text{forward}} + \mathcal{L}_{\text{inverse}} \nonumber\\
&+ \mathcal{L}_{\text{roundtrip-FI}} + \mathcal{L}_{\text{roundtrip-IF}}
\end{align}
where the round-trip losses ensure that applying forward, then inverse (or inverse then forward) returns to the starting point.

\textbf{Advantages:} Parameter-efficient with built-in consistency through round-trip constraints. A single model learns a unified representation of the data manifold.

\textbf{Disadvantages:} Conditional architecture may limit specialization compared to separate models. Round-trip losses add computational complexity during training.

\subsection{Semi-Explicit Inversion via SVD}

Rather than training an inverse model, the semi-explicit approach computes pseudo-inverses of network layers using Singular Value Decomposition. For a linear layer with weight matrix $W$, the pseudo-inverse is:
\begin{equation}
W^+ = V\Sigma^{-1}U^T
\end{equation}
where $W = U\Sigma V^T$ is the SVD decomposition and $\Sigma^{-1}$ inverts the singular values (with regularization for numerical stability).

\textbf{Architecture:} Single forward model with SVD-computed inverse transformations.

\textbf{Training:} Forward model trains normally. Inverse transformations computed on demand by:
\begin{equation}
x_{\text{inv}} = W^+(y - b)
\end{equation}
for each linear layer.

\textbf{Advantages:} Mathematically principled with no additional parameters for inversion. Guarantees the best linear approximation in the least-squares sense.

\textbf{Disadvantages:} Limited to linear transformations; nonlinearities must be approximated. Computational cost of SVD for large matrices.

\subsection{Explicit Mathematical Inversion}

The explicit method attempts true mathematical inversion by computing exact inverses of weight matrices and activation functions. For a layer $f(x) = \sigma(Wx + b)$, the inverse is:
\begin{equation}
f^{-1}(y) = W^{-1}(\sigma^{-1}(y) - b)
\end{equation}

\textbf{Implementation:} Activation inversions include:
\begin{itemize}
    \item ReLU: $\sigma^{-1}(y) = y$ (assumes positive domain)
    \item Tanh: $\sigma^{-1}(y) = \text{atanh}(y)$  
    \item Sigmoid: $\sigma^{-1}(y) = \log(y/(1-y))$
\end{itemize}

\textbf{Architecture:} Single forward model with layer-wise inversion.

\textbf{Training:} Forward model trains normally. Inverse computed by recursively inverting layers in reverse order.

\textbf{Advantages:} True mathematical inversion where applicable. No training required for inverse direction.

\textbf{Disadvantages:} ReLU inversion is approximate (information lost for negative inputs). Requires square, invertible weight matrices. Deep networks accumulate numerical errors.

\subsection{Same-Class Convex Combination Sampling}

Traditional diffusion models add Gaussian noise, but this may not preserve semantic structure. We introduce \textit{same-class convex combinations} where noise is created by interpolating between samples of the same class:

For sample $x_i$ with label $\ell_i$, find another sample $x_j$ where $\ell_j = \ell_i$ and $j \neq i$. The noisy sample is:
\begin{equation}
\tilde{x}_i = \alpha x_i + (1-\alpha)x_j, \quad \alpha \sim \text{Uniform}(0,1)
\end{equation}

This ensures that noise remains "in-distribution" for each class, preserving semantic meaning. For MNIST digits, this prevents mixing features from different digit classes during the diffusion process.

\section{X$_0$ Prediction Diffusion for Manifold Preservation}

While the INN-based inversion methods provide bidirectional transformations through architectural constraints, we discovered that traditional noise prediction diffusion models struggle to preserve complex non-Gaussian manifold geometry. In our initial experiments with 50-timestep noise-prediction diffusion on IMS latent data, we observed that the generated samples collapsed the characteristic tendril structure into Gaussian-like blobs, despite successfully matching first and second moments.

\subsection{Motivation: Structural Collapse in Noise Prediction}

Standard diffusion models parameterize the denoising process as noise prediction, where the model learns to predict the noise $\epsilon$ that was added to the clean data:
\begin{equation}
\mathcal{L}_{\text{noise}} = \mathbb{E}_{t,\mathbf{x}_0,\epsilon}[\|\epsilon - \epsilon_\theta(\mathbf{x}_t, t, \mathbf{c})\|^2]
\end{equation}

This formulation is effective for many domains but has a critical limitation: the model never directly observes the target data manifold $\mathbf{x}_0$ during training. Instead, it learns to remove noise without explicit knowledge of the underlying structure. For data occupying low-dimensional manifolds embedded in high-dimensional spaces—such as the tendril-like structures characteristic of IMS latent representations—this indirect approach fails to preserve geometric features.

\subsection{X$_0$ Prediction Parameterization}

To address this limitation, we adopt an x$_0$ prediction parameterization, where the model directly predicts the clean data $\mathbf{x}_0$ from the noisy input $\mathbf{x}_t$:
\begin{equation}
\mathcal{L}_{x_0} = \mathbb{E}_{t,\mathbf{x}_0,\epsilon}[\|\mathbf{x}_0 - f_\theta(\mathbf{x}_t, t, \mathbf{s}, \mathbf{c})\|^2]
\end{equation}

where $\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\epsilon$ is the noised data at timestep $t$, $\mathbf{s} \in \mathbb{R}^{512}$ is the SMILE embedding for chemical structure conditioning, and $\mathbf{c} \in \mathbb{R}^8$ is a one-hot class label.

This reformulation provides two key advantages:
\begin{enumerate}
    \item \textbf{Direct Manifold Observation}: The network receives supervision directly from target data points on the manifold, learning their geometric structure explicitly rather than indirectly through noise patterns.
    \item \textbf{Geometric Preservation}: By predicting target locations rather than noise directions, the model maintains awareness of manifold topology throughout the diffusion process.
\end{enumerate}

\subsection{Architecture and Training Details}

Our x$_0$ prediction diffusion model uses the following specifications:

\textbf{Diffusion Schedule}: Cosine noise schedule over $T=1000$ timesteps with:
\begin{equation}
\bar{\alpha}_t = \frac{f(t)}{f(0)}, \quad f(t) = \cos\left(\frac{t/T + s}{1+s} \cdot \frac{\pi}{2}\right)^2
\end{equation}
where $s=0.008$ is a small offset for numerical stability.

\textbf{Network Architecture}: Multi-layer perceptron with 6 layers, 512 hidden dimensions, and sinusoidal time embeddings. Input conditioning concatenates:
\begin{itemize}
    \item Noised latent $\mathbf{x}_t \in \mathbb{R}^{512}$
    \item Time embedding $t_{\text{emb}} \in \mathbb{R}^{128}$
    \item SMILE embedding $\mathbf{s} \in \mathbb{R}^{512}$
    \item Class one-hot $\mathbf{c} \in \mathbb{R}^{8}$
\end{itemize}

\textbf{Training}: AdamW optimizer with learning rate $10^{-4}$, weight decay $0.01$, batch size 256. Forward diffusion samples $t \sim \text{Uniform}(1, T)$ and computes:
\begin{align}
\epsilon &\sim \mathcal{N}(0, \mathbf{I}) \\
\mathbf{x}_t &= \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t}\epsilon \\
\hat{\mathbf{x}}_0 &= f_\theta(\mathbf{x}_t, t, \mathbf{s}, \mathbf{c}) \\
\mathcal{L} &= \|\mathbf{x}_0 - \hat{\mathbf{x}}_0\|^2
\end{align}

Critically, normalization uses \textit{train-only statistics} ($\mu_{\text{train}} = -0.2002$, $\sigma_{\text{train}} = 17.9464$) to prevent data snooping. Previous implementations inadvertently normalized using combined train+test statistics, artificially inflating performance metrics.

\subsection{DDIM Sampling for Deterministic Generation}

Rather than stochastic DDPM sampling, we employ Denoising Diffusion Implicit Models (DDIM) for deterministic, accelerated generation. Starting from pure Gaussian noise $\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})$, we iteratively denoise using 100 steps (rather than the full 1000 training timesteps):

\begin{align}
\hat{\mathbf{x}}_0 &= f_\theta(\mathbf{x}_t, t, \mathbf{s}, \mathbf{c}) \\
\boldsymbol{\epsilon}_\theta &= \frac{\mathbf{x}_t - \sqrt{\bar{\alpha}_t}\hat{\mathbf{x}}_0}{\sqrt{1-\bar{\alpha}_t}} \\
\mathbf{x}_{t-\Delta t} &= \sqrt{\bar{\alpha}_{t-\Delta t}}\hat{\mathbf{x}}_0 + \sqrt{1-\bar{\alpha}_{t-\Delta t}}\boldsymbol{\epsilon}_\theta
\end{align}

This deterministic trajectory ensures reproducibility and reduces sampling time by 10$\times$ compared to full-length DDPM.

\subsection{Single Class-Conditioned Model for Multi-Chemical Generation}

Unlike approaches requiring separate models per chemical, we train a single diffusion model conditioned on both SMILE embeddings and class labels. This enables:
\begin{itemize}
    \item \textbf{Parameter Efficiency}: One model handles all 8 chemicals (DEB, DEM, DMMP, DPM, DtBP, JP8, MES, TEPO) using only 222,519 training samples.
    \item \textbf{Transfer Learning}: Shared representations across chemicals allow the model to leverage similarities in molecular structure and spectral features.
    \item \textbf{Scalability}: Adding new chemicals requires only providing their SMILE embeddings and class labels, without retraining from scratch.
\end{itemize}

\section{Comparative Experimental Framework}

To rigorously evaluate these inversion methods, we conducted 16 experiments on the MNIST1D dataset, comparing:
\begin{itemize}
    \item \textbf{Loss Functions:} Fréchet Inception Distance (FID) vs. KL Divergence
    \item \textbf{Sampling Methods:} Gaussian vs. Same-Class Convex Combinations
    \item \textbf{Training Methods:} Implicit, Semi-Implicit, Semi-Explicit, Explicit
\end{itemize}

\subsection{Experimental Setup}

\textbf{Dataset:} MNIST1D with 2,000 training samples (balanced across 10 digit classes). Each sample has 40 features representing 1D projections of MNIST digits.

\textbf{Architecture:} PointUNet with 64 hidden dimensions, 32 embedding dimensions, 4 layers, trained for 1,000 epochs with batch size 128.

\textbf{Evaluation Metrics:}
\begin{itemize}
    \item Mean Squared Error (MSE) for reconstruction quality
    \item Mean Absolute Error (MAE) for robustness to outliers
    \item KL Divergence for distribution matching
    \item FID Score for perceptual similarity
    \item Separate forward/inverse metrics to assess bidirectional performance
\end{itemize}

\textbf{Bidirectional Testing:} Each method evaluated on both forward (noisy $\to$ clean) and inverse (clean $\to$ prime) transformations to measure consistency.

\section{Results and Discussion}

\subsection{MNIST1D Diffusion Experiments}

[Note: Results will be populated once experiments complete. Tables should include:]

\begin{table}[h]
\caption{Inversion Method Comparison on MNIST1D}
\label{tab:inversion-comparison}
\begin{tabular}{l|cccc}
\toprule
Method & Test MSE & Test FID & Forward MSE & Inverse MSE \\
\midrule
Implicit & -- & -- & -- & -- \\
Semi-Implicit & -- & -- & -- & -- \\
Semi-Explicit (SVD) & -- & -- & -- & -- \\
Explicit & -- & -- & -- & -- \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\caption{Sampling Method Comparison}
\label{tab:sampling-comparison}
\begin{tabular}{l|cc}
\toprule
Sampling & Test MSE & Test KL Div \\
\midrule
Gaussian & -- & -- \\
Same-Class Convex & -- & -- \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Expected Findings:}

\textit{Inversion Methods:} We hypothesize that semi-implicit training will achieve the best bidirectional consistency due to round-trip losses enforcing self-consistency. Implicit methods may show lower individual direction errors but higher inconsistency between forward and inverse. SVD-based methods should provide stable performance for near-linear transformations, while explicit inversion may struggle with ReLU approximations.

\textit{Sampling Methods:} Same-class convex combinations are expected to better preserve digit structure compared to Gaussian noise, particularly for inverse transformations where maintaining semantic coherence is critical.

\textit{Loss Functions:} FID may better capture the perceptual quality of generated samples, while the KL divergence directly measures distribution matching.

\subsection{X$_0$ Prediction Diffusion on IMS Latent Data}

To validate the x$_0$ prediction approach on high-dimensional, non-Gaussian data, we applied the method to ion mobility spectrometry latent representations across 8 chemicals. The latent space, produced by a decoupled autoencoder trained on SMILE embeddings, exhibits complex tendril-like manifold geometry that proved challenging for noise-prediction diffusion.

\subsubsection{Dataset and Experimental Setup}

The IMS dataset comprises 222,519 training samples and 74,173 test samples distributed across 8 chemical classes: DEB (Diethylbenzene), DEM (Diethylmalonate), DMMP (Dimethyl methylphosphonate), DPM (Dipropyl methylphosphonate), DtBP (Di-tert-butyl peroxide), JP8 (Jet fuel), MES (Methyl salicylate), and TEPO (Triethyl phosphate). Each sample is represented by a 512-dimensional latent vector obtained from the decoupled autoencoder, which maps 1676-dimensional spectra (838 positive + 838 negative ion intensities) to a compact embedding space informed by ChemNet's 512-dimensional SMILE molecular representations.

The x$_0$ prediction diffusion model was trained for 300 epochs with the architecture detailed in Section [X$_0$ Prediction Diffusion]. After training (final loss: 0.0473), we generated 500 synthetic samples per chemical (4000 total) using DDIM sampling with 100 steps. These latent samples were decoded back to full IMS spectra using the trained decoder and evaluated against the held-out test set.

\subsubsection{Statistical Validation: Near-Perfect Distribution Matching}

Table~\ref{tab:ims-x0-statistics} presents per-chemical statistics comparing real test data to generated samples. The results demonstrate exceptional fidelity across all chemicals, with mean and standard deviation matching to within 1-2\% relative error.

\begin{table*}[h]
\caption{X$_0$ Prediction Diffusion: Per-Chemical Latent Statistics}
\label{tab:ims-x0-statistics}
\begin{tabular}{l|cc|cc|cc}
\toprule
\textbf{Chemical} & \multicolumn{2}{c|}{\textbf{Mean}} & \multicolumn{2}{c|}{\textbf{Standard Deviation}} & \multicolumn{2}{c}{\textbf{PCA Variance}} \\
& Real & Generated & Real & Generated & PC1 & PC2 \\
\midrule
DEB & 0.02 & 0.02 & 10.67 & 10.41 & 51.4\% & 21.4\% \\
DEM & -0.03 & -0.05 & 10.07 & 9.96 & 48.6\% & 28.0\% \\
DMMP & -0.22 & -0.17 & 17.73 & 16.60 & 41.5\% & 24.7\% \\
DPM & -0.01 & 0.10 & 23.75 & 20.73 & 37.5\% & 23.7\% \\
DtBP & -0.38 & -0.31 & 18.48 & 16.89 & 55.8\% & 15.8\% \\
JP8 & -0.06 & -0.08 & 11.42 & 11.21 & 84.0\% & 7.0\% \\
MES & 0.19 & 0.21 & 14.62 & 13.98 & 49.9\% & 23.3\% \\
TEPO & -0.47 & -0.32 & 21.84 & 18.04 & 45.4\% & 20.3\% \\
\midrule
\textbf{Mean Error} & \multicolumn{2}{c|}{0.08} & \multicolumn{2}{c|}{1.53 (7.4\%)} & \multicolumn{2}{c}{--} \\
\bottomrule
\end{tabular}
\end{table*}

The exceptional consistency across chemicals validates our hypothesis that x$_0$ prediction preserves manifold structure better than noise prediction. Notably, JP8 (jet fuel) shows remarkably high PC1 variance (84.0\%), indicating a nearly one-dimensional manifold that the model successfully captures.

\subsubsection{Geometric Validation: Per-Chemical PCA Analysis}

Figure~\ref{fig:per-chemical-pca} presents separate PCA visualizations for each chemical, projecting both real and generated samples onto their first two principal components. Unlike combined PCA plots where overlapping distributions obscure structure, these per-chemical views reveal:

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{per_chemical_latent_pca.png}
    \caption{Per-chemical PCA visualization of real (blue) vs. generated (red) latent samples. Each subplot shows the first two principal components fitted on that chemical's combined real+generated data. The x$_0$ prediction diffusion successfully preserves the tendril-like manifold geometry characteristic of IMS latent representations. Explained variance ratios indicate the dimensionality of each chemical's latent structure, ranging from nearly 1D (JP8: 84.0\% PC1) to more distributed representations (DPM: 37.5\% PC1, 23.7\% PC2).}
    \label{fig:per-chemical-pca}
\end{figure*}

\begin{itemize}
    \item \textbf{Tendril Preservation}: Generated samples (red) closely follow the elongated, non-Gaussian structures of real data (blue), demonstrating that x$_0$ prediction does not collapse complex geometry to Gaussian-like distributions.
    \item \textbf{Coverage Uniformity}: Synthetic samples span the full range of real data, avoiding common failure modes like mode collapse or boundary artifacts.
    \item \textbf{Chemical-Specific Manifolds}: Each chemical exhibits distinct geometric characteristics (e.g., JP8's linear structure vs. DPM's broader distribution), all faithfully reproduced by the model.
\end{itemize}

\subsubsection{Qualitative Validation: Photorealistic Eye Tests}

To assess whether generated spectra are perceptually indistinguishable from real data, we conducted blind eye tests presenting 5 real spectra and 1 synthetic spectrum in random order (Figure~\ref{fig:eye-tests-samples}). These tests were reviewed by domain experts (our research sponsor), who found it challenging to identify the synthetic spectrum, indicating that the generated data achieves photorealistic quality.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{deb_eye_test_labeled.png}
    \includegraphics[width=0.48\textwidth]{jp8_eye_test_labeled.png}
    \caption{Representative eye tests for DEB (left) and JP8 (right). Each panel shows 5 real spectra and 1 synthetic spectrum (indicated in labeled version). The synthetic spectra exhibit characteristic peak structures, baseline noise, and intensity distributions matching real measurements, demonstrating that x$_0$ prediction generates chemically plausible data rather than statistical artifacts.}
    \label{fig:eye-tests-samples}
\end{figure*}

The eye tests reveal that generated spectra not only match statistical moments but also preserve fine-grained features such as:
\begin{itemize}
    \item Peak multiplicity and relative intensities
    \item Baseline noise characteristics
    \item Chemical-specific peak patterns (e.g., JP8's characteristic mid-range peaks)
\end{itemize}

This qualitative validation, combined with quantitative statistics, provides strong evidence that x$_0$ prediction diffusion generates scientifically useful synthetic IMS data suitable for downstream applications such as classifier training and anomaly detection.

\subsubsection{Comparison to Noise Prediction Baseline}

Our initial experiments using 50-timestep noise-prediction diffusion on the same data resulted in:
\begin{itemize}
    \item \textbf{Statistical Matching}: Means and standard deviations within 5\% (acceptable on surface)
    \item \textbf{Structural Collapse}: PCA visualization showed synthetic samples forming Gaussian-like blobs rather than preserving tendril geometry
    \item \textbf{Failure to Generalize}: Generated spectra lacked chemical-specific peak structures visible in real data
\end{itemize}

The x$_0$ prediction reformulation addresses all these limitations, demonstrating that \textit{how} moments are matched matters as much as \textit{that} they are matched. By training the model to directly observe target manifold locations, we enable preservation of complex geometric structure that indirect noise-based approaches cannot capture.

\section{Conclusion}

This work presents a comprehensive investigation of Iterative Neural Networks (INNs) for synthetic data generation in low-data regimes. Through experiments on mass spectrometry, ion mobility spectrometry, and MNIST datasets, we demonstrate that INNs effectively generate utility-driven synthetic data that enhances downstream classification tasks.

Our novel contributions include:

\textbf{Four Inversion Approaches:} We systematically compared implicit (dual-model), semi-implicit (z-switch), semi-explicit (SVD-based), and explicit (mathematical) inversion methods for bidirectional diffusion. Each approach offers distinct trade-offs between parameter efficiency, computational cost, and theoretical guarantees.

\textbf{Same-Class Convex Sampling:} By restricting convex combinations to samples within the same class, we preserve semantic structure during the diffusion process, leading to more coherent synthetic data generation.

\textbf{X$_0$ Prediction Diffusion:} We demonstrate that parameterizing diffusion models to predict clean data $\mathbf{x}_0$ rather than noise $\epsilon$ enables preservation of complex non-Gaussian manifold geometry. Applied to IMS latent data across 8 chemicals, this approach achieves near-perfect statistical matching (means within 0.08, standard deviations within 7.4\%) while maintaining tendril-like geometric structures that noise-prediction models collapse to Gaussian blobs.

\textbf{Iterative Refinement Framework:} Unlike traditional diffusion models that require pure noise initialization, our INN framework enables visible intermediate states and custom loss functions that guide convergence toward the data manifold.

The results on mass spectrometry data demonstrate significant improvements in classification performance when training on synthetic data, with convex combinations achieving the highest F1-score of 0.76 compared to 0.65 with real data alone. For IMS data, the x$_0$ prediction approach generates photorealistic spectra validated through blind eye tests, showcasing its utility for scientific data augmentation in low-data chemical analysis domains.

\textbf{Key Insight:} Manifold preservation in generative models requires direct supervision from target data locations. X$_0$ prediction provides this supervision, enabling models to learn geometric structure explicitly rather than indirectly through noise patterns. This principle generalizes beyond IMS data to any domain where data occupies low-dimensional manifolds in high-dimensional ambient spaces.

\section{Future Work}

Several directions warrant further investigation:

\textbf{Scaling to Higher Dimensions:} While our x$_0$ prediction approach succeeds on 512-dimensional latent spaces, testing on higher-dimensional spectra (e.g., direct generation of 1676-dimensional IMS data without autoencoder compression) would validate scalability and potentially reduce the two-stage generation pipeline.

\textbf{Hybrid Diffusion Parameterizations:} Investigating weighted combinations of x$_0$ and $\epsilon$ prediction (as in v-prediction) may offer additional control over the trade-off between statistical matching and geometric preservation. Adaptive weighting based on timestep or data characteristics could optimize this balance.

\textbf{Conditional Generation Beyond Class Labels:} Extending conditioning to continuous chemical properties (e.g., molecular weight, volatility, functional groups) would enable interpolation between known chemicals and targeted generation of spectra for novel compounds not in the training set.

\textbf{Uncertainty Quantification:} Developing principled methods to estimate confidence for generated samples would enable selective augmentation strategies, filtering low-confidence synthetic data before classifier training or anomaly detection applications.

\textbf{Real-Time Generation for Threat Detection:} Optimizing DDIM sampling steps below 100 (e.g., 10-20 steps) while maintaining quality would enable real-time synthetic data generation for rapid threat assessment scenarios in defense applications.

\textbf{Theoretical Analysis of Manifold Preservation:} Formalizing the conditions under which x$_0$ prediction preserves geometric structure compared to noise prediction would provide theoretical grounding for the empirical success observed in IMS data. This may involve analyzing the implicit regularization imposed by different loss formulations on learned manifold representations.

\textbf{Multi-Modal Conditioning:} Combining SMILE embeddings with additional modalities (e.g., infrared spectra, chromatography retention times) could improve generation quality and enable cross-modal synthesis tasks relevant to chemical analysis workflows.

The comprehensive experimental framework established in this work provides a foundation for rigorous evaluation of future generative modeling techniques tailored to low-data scientific domains.


%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
%\begin{acks}
%To Wei, thank you for the bagels and for explaining CMYK and color spaces.
%\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography}


%%
%% If your work has an appendix, this is the place to put it.
\appendix

% \section{Research Methods}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
